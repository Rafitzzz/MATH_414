In this section, we explore how random (you'll see that they're not so random) numbers are generated in our computers and how the programs designed for creating these numbers work.

\begin{definition}
    A random number generatorn (RNG) is a procedure that produces an \textit{infinite} stream of independent, identically distributed (i.i.d.) random variables $U_1, U_2, \ldots \sim \mu$
    accoording to some \textit{probability distribution} $\mu$.
\end{definition}

Recall that a probability dustribution is basically a fancy way of saying “how likely different things are to happen.” Imagine you have a bag of Skittles, and you want to know the chances of grabbing each color. 
A probability distribution is like a chart or rulebook that says: `Red has a 30\% chance, green has 20\%, purple is rare like a unicorn at 5\%, etc.'

\begin{remark}
    Note that an RNG is a \textit{uniform} random number generator if $\mu$ is the \textit{uniform distribution} in $(0,1)$. Where the uniform distribution is the “everyone gets a fair share” version of probability. Every outcome has the same chance of happening.
\end{remark}

Since all of the current RNG's are based on algorithms, they produce a \textit{purely deterministic} (i.e. the outcome is already locked in once you know the rules) stream of variables $U_1, U_2, U_3, \ldots$ which `look like' a stream of i.i.d. random variables. 
For this reason, algorithmic generators are called pseudo-random number generators. Here's a lil' sexy pseudo code for how such implementation of an RNG could go about:
\vspace{0.2cm}

Let $\mathcal{S}$ be a finite state space and $\mathcal{U}$ be the output space. Let $f: \mathcal{S} \to \mathcal{S}$ (i.e. a function that `evolves' the system, goes from one state to annother) and $g: \mathcal{S} \to \mathcal{U}$ be given functions.

\vspace{0.2cm}

\textbf{Algorithm: Pseudo-RNG}
\begin{enumerate}
    \item take $X_0 \in \mathcal{S}$ \quad // seed
    \item for $k=1, 2, \dots$ do
    \item $X_k = f(X_{k-1})$ // recursion on state variable $X_k \in \mathcal{S}$
    \item $U_k = g(X_k)$ // output $U_k \in \mathcal{U}$
    \item end
\end{enumerate}

\begin{remark}
    Important notions to notice:
\begin{itemize}
    \item $X_0$ is called the seed.
    \item A Pseudo-RNG starting from a given seed will always produce the same sequence $U_1, U_2, \dots$.
    \item Since the state space $\mathcal{S}$ is finite, the generator eventually revisits an already visited state. All Pseudo-RNGs are \textit{periodic} (i.e. eventually return to the initial state).
    \item Good generators have period $p = |\mathcal{S}|$.
\end{itemize}
\end{remark}

\vspace{0.2cm}

Given the information we now know, we can \textit{compress} some of the `good' qualities a uniform pseudo RNG might possess:
\begin{itemize}
        \item Have a large period
        \item Pass a battery of statistical tests for uniformity and independence.
        \item Be fast and efficient
        \item Be reproducible
        \item Have the possibility to generate multiple streams.
        \item Avoid producing the numbers 0 and 1.
\end{itemize} 

We would now like to evaluate if these generators produce pure bullshit or if they're actually giving us sequences that resemble what we're looking for; an i.i.d. sequence of random variables accoording to some probability distribution.

\begin{remark}
    The Cummulative Distribution Function (CDF) denotes the probability that a random variable $X$ is less than or equal to some value $x$. Formally speaking,
    $$\text{CDF}(X) := \mathbb{P}(X \leq x)$$
    
    In other words, instead of just looking at one outcome, it keeps a running total of probabilities up to that point. 
    \begin{itemize}
        \item It always starts at 0 (nothing less than the smallest value has happened yet).
        \item Ends at 1 (because by the time you’ve included all possible values, the probability is 100\%).
    \end{itemize} 
    So, while a probability distribution $\mathbb{P}(X=x)$ shows “the chance of this exact outcome,” the CDF shows “the chance of being at or below this outcome.”
\end{remark}

\vspace{0.2cm}

The framework we're working with is the following. Let $U \in I \subset \mathbb{R}$ be a random variable with $\text{CDF}(X) = F(x) = \mathbb{P}(U \leq x)$. We define $U := \{U_1, \ldots, U_n\}$
to be a sample produced by a RNG with empirical distribution function $\hat{F}(x)$.  

\begin{remark}
    The empirical distribution function is basically the "data-based version" 
of the cumulative distribution function (CDF). Instead of coming from a 
theoretical model, it is built directly from a sample of data.

For a sample \(X_1, X_2, \ldots, X_n\), the empirical distribution function is:

\[
\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} \mathds{1}_{\{X_i \leq x\}} = \frac{\text{number of } \{U_i \leq x\}}{n}
\]

where \(n\) = numaber of data points, \(\mathds{1}_{\{X_i \leq x\}}\) is an indicator function (it equals 1 if 
\(X_i \leq x\), and 0 otherwise). So, \(\hat{F}(x)\) is just the proportion of sample points that are 
less than or equal to \(x\).  
\end{remark}

\vspace{0.2cm}

One of the things statisticians love to do is to test the hypothesis $H_0$ that $U$ has been drawn independently from the distribution $F$. This would tell us if
each $U_i$ does not depend form any other $U_j (\text{ }\forall i \neq j)$ and so, giving us confidence that our data is `random' in some sense. Annother thing we would also like to know, is to decide if our empirical data $U$ generated by a RNG, actually `fits' with the theoretical data. In other words,
we would like to see if $U$ matches $F$. We begin by exploring some non-parametric (i.e. we do not rely on parameters like mean or variance, just pure raw data son) goodness-of-fit tests for a given sample sequence $U$.

\begin{enumerate}
    \item \textbf{Q-Q Plot (Quantile v. Quantile plot): } 
    Imagine you have two classes of students. You suspect they both come from a population with the same height distribution. To check, you do the following:
        \begin{enumerate}
            \item Line up all the students in \textbf{Class A} (your generated data) from shortest to tallest.
            \item Do the same for \textbf{Class B} (the perfect, theoretical distribution).
            \item Compare them one-on-one: the shortest from A vs. the shortest from B, the second shortest vs. the second shortest, and so on.
        \end{enumerate}

        If you plot their heights against each other (Class A's heights on the y-axis, Class B's on the x-axis), and they really do have the same distribution, the points should form a perfectly straight diagonal line. The shortest students are about the same height, the median students are the same height, and the tallest students are the same height. 
        If, however, Class A was secretly full of basketball players, their heights would be consistently taller than Class B's for the same rank, and the line would curve away (the Q-Q plot does exactly this, but with data points instead of people!).

        \vspace{0.2cm}

        A Q-Q plot compares the quantiles of our data's empirical distribution $\hat{F}_n$ versus the quantiles of the theoretical distribution $F$. \begin{definition} A \textit{quantile} is just a fancy word for a point below which a certain fraction of the data falls. For instance, the 0.25 quantile (or 25th percentile) 
        is the value that is greater than 25\% of the data. 

        Mathematically, the lecture defines the quantile from our data as the estimator $\hat{q}_p = U^{(j)}$ and the exact theoretical quantile as $q_p = \text{argmin}_x \{F(x) \ge p\}$, where $p=\frac{j}{n+1}$. We plot $\hat{q}_p$ (on the y-axis) against $q_p$ (on the x-axis). \end{definition}

        \textbf{The Verdict}
        After plotting all the points, we simply look at the result:
        \begin{itemize}
            \item If the points form a nice, straight line along $y=x$, we pop the champagne! Our data is a good fit for the theoretical distribution. We can trust our random number generator.
            \item If the points deviate from the line in a systematic pattern (like an S-curve or a banana shape), our generator is fucking up. The shape of the deviation can even tell us \textit{how} our data is different (e.g., if it has ``heavier tails'').
        \end{itemize}

    \item \textbf{Kolmogorov-Smirnov Test: }
    This non-parametric statistical tool is used to determine if a given data sample comes from a specific continuous distribution. In essence, it quantifies the maximum vertical distance between the distribution of our data and the ideal distribution we are comparing it against.

    \vspace{1em}
    \noindent\textbf{The Null Hypothesis ($H_0$)}
    \vspace{0.5em}

    The core assumption we are testing is the null hypothesis, $H_0$: The data sample $(U_1, \dots, U_n)$ was drawn independently from the specified theoretical continuous distribution with CDF $F(x)$.

    \vspace{1em}
    \noindent\textbf{The Test Statistic ($D_n$)}
    \vspace{0.5em}

    The test is built around the K-S statistic, $D_n$, which measures the largest absolute difference between the \textbf{empirical CDF ($\hat{F}_n(x)$)} of the sample and the \textbf{theoretical CDF ($F(x)$)}. The test statistic is then the supremum (for a finite sample, the maximum) of the absolute difference across all possible values of $x$:
    \[
    D_n = \sup_{x} |\hat{F}_n(x) - F(x)|
    \]

    \vspace{1em}
    \noindent\textbf{The Decision Rule}
    \vspace{0.5em}

    Under the null hypothesis, the distribution of the scaled statistic, $\sqrt{n}D_n$, converges to a known distribution called the Kolmogorov distribution, regardless of what $F(x)$ is. This powerful result allows us to perform a standardized test.

    We reject the null hypothesis $H_0$ at a significance level $\alpha$ if our calculated statistic is greater than a critical value $K_\alpha$ obtained from tables of the Kolmogorov distribution.
    \[
    \text{If } \sqrt{n}D_n > K_\alpha, \text{ reject } H_0.
    \]
    If we reject $H_0$, we have statistical evidence that our data sample does not follow the theoretical distribution $F$. Otherwise, if $\sqrt{n}D_n \le K_\alpha$, we fail to reject $H_0$, meaning our data is consistent with the proposed distribution.

\end{enumerate}